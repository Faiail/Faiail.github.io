---
title: "Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts"
collection: publications
category: conferences
permalink: /publication/label-anything
excerpt: 'This paper introduces a novel method for multi-class few-shot semantic segmentation, enabling users to provide different prompt types, such as masks, bounding boxes, and points.'
date: 2025-10-25
venue: '28th International Conference on Artificial Intelligence'
paperurl: 'https://arxiv.org/pdf/2407.02075'
citation: 'De Marinis, P., Fanelli N., Scaringi, R., Colonna, E., Fiameni, G., Vessio, G., Castellano, G. (2025) &quot;Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts&quot; <i>ECAI 2025</i>.'
---

Few-shot semantic segmentation aims to segment objects from previously unseen classes using only a limited number of labeled examples. In this paper, we introduce Label Anything, a novel transformer-based architecture designed for multi-prompt, multi-way few-shot semantic segmentation. Our approach leverages diverse visual prompts---points, bounding boxes, and masks---to create a highly flexible and generalizable framework that significantly reduces annotation burden while maintaining high accuracy. Label Anything makes three key contributions: (*i*) we introduce a new task formulation that relaxes conventional few-shot segmentation constraints by supporting various types of prompts, multi-class classification, and enabling multiple prompts within a single image; (*ii*) we propose a novel architecture based on transformers and attention mechanisms; and (*iii*) we design a versatile training procedure allowing our model to operate seamlessly across different $N$-way $K$-shot and prompt-type configurations with a single trained model. Our extensive experimental evaluation on the widely used COCO-$20^i$ benchmark demonstrates that Label Anything achieves state-of-the-art performance among existing multi-way few-shot segmentation methods, while significantly outperforming leading single-class models when evaluated in multi-class settings. Code and trained models are available at [](https://github.com/pasqualedem/LabelAnything).
